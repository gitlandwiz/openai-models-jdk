/**
* OpenAI API
* APIs for sampling from and fine-tuning language models
*
* The version of the OpenAPI document: 1.2.0
* 
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
package org.openapitools.client.models

import org.openapitools.client.models.ChatCompletionRequestMessage
import org.openapitools.client.models.OneOfLessThanStringCommaArrayGreaterThan

import com.squareup.moshi.Json

/**
 * 
 * @param model ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.
 * @param messages The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).
 * @param temperature completions_temperature_description
 * @param topP completions_top_p_description
 * @param n How many chat completion choices to generate for each input message.
 * @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. 
 * @param stop Up to 4 sequences where the API will stop generating further tokens. 
 * @param maxTokens The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). 
 * @param presencePenalty completions_presence_penalty_description
 * @param frequencyPenalty completions_frequency_penalty_description
 * @param logitBias Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
 */

data class CreateChatCompletionRequest (
    /* ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported. */
    @Json(name = "model")
    val model: kotlin.String,
    /* The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction). */
    @Json(name = "messages")
    val messages: kotlin.collections.List<ChatCompletionRequestMessage>,
    /* completions_temperature_description */
    @Json(name = "temperature")
    val temperature: java.math.BigDecimal? = null,
    /* completions_top_p_description */
    @Json(name = "top_p")
    val topP: java.math.BigDecimal? = null,
    /* How many chat completion choices to generate for each input message. */
    @Json(name = "n")
    val n: kotlin.Int? = null,
    /* If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.  */
    @Json(name = "stream")
    val stream: kotlin.Boolean? = null,
    /* Up to 4 sequences where the API will stop generating further tokens.  */
    @Json(name = "stop")
    val stop: OneOfLessThanStringCommaArrayGreaterThan? = null,
    /* The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).  */
    @Json(name = "max_tokens")
    val maxTokens: kotlin.Int? = null,
    /* completions_presence_penalty_description */
    @Json(name = "presence_penalty")
    val presencePenalty: java.math.BigDecimal? = null,
    /* completions_frequency_penalty_description */
    @Json(name = "frequency_penalty")
    val frequencyPenalty: java.math.BigDecimal? = null,
    /* Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  */
    @Json(name = "logit_bias")
    val logitBias: kotlin.Any? = null
)

